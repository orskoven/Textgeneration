{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "import-libraries",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/simonbeckmann/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/simonbeckmann/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from textblob import TextBlob\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-corpus",
   "metadata": {},
   "source": [
    "### Load the Corpus of Comedy Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "load-pickle-file",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'comedy_scripts.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the corpus of comedy scripts from a pickle file\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcomedy_scripts.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m     scripts \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Create a DataFrame for easier manipulation\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf-metal/lib/python3.9/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'comedy_scripts.pkl'"
     ]
    }
   ],
   "source": [
    "# Load the corpus of comedy scripts from a pickle file\n",
    "with open('comedy_scripts.pkl', 'rb') as f:\n",
    "    scripts = pickle.load(f)\n",
    "\n",
    "# Create a DataFrame for easier manipulation\n",
    "df = pd.DataFrame(scripts, columns=['comedian', 'script'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sentiment-analysis",
   "metadata": {},
   "source": [
    "### Sentiment Analysis Using TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "calculate-sentiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate polarity and subjectivity\n",
    "def get_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity, blob.sentiment.subjectivity\n",
    "\n",
    "# Apply the function to each script\n",
    "df[['polarity', 'subjectivity']] = df['script'].apply(lambda x: pd.Series(get_sentiment(x)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-sentiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot polarity vs subjectivity\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['polarity'], df['subjectivity'], alpha=0.5)\n",
    "plt.title('Polarity vs Subjectivity of Comedians')\n",
    "plt.xlabel('Polarity')\n",
    "plt.ylabel('Subjectivity')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sentiment-over-time",
   "metadata": {},
   "source": [
    "### Analyze Sentiment Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-scripts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into parts\n",
    "def split_text(text, n_parts):\n",
    "    words = text.split()\n",
    "    length = len(words)\n",
    "    parts = []\n",
    "    for i in range(n_parts):\n",
    "        start = int(i * length / n_parts)\n",
    "        end = int((i + 1) * length / n_parts)\n",
    "        part = ' '.join(words[start:end])\n",
    "        parts.append(part)\n",
    "    return parts\n",
    "\n",
    "n_parts = 12\n",
    "sentiment_over_time = defaultdict(list)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    parts = split_text(row['script'], n_parts)\n",
    "    for part in parts:\n",
    "        polarity, subjectivity = get_sentiment(part)\n",
    "        sentiment_over_time[row['comedian']].append(polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-sentiment-over-time",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sentiment over time for each comedian\n",
    "plt.figure(figsize=(12, 8))\n",
    "for comedian, sentiments in sentiment_over_time.items():\n",
    "    plt.plot(range(n_parts), sentiments, label=comedian)\n",
    "plt.title('Sentiment Over Time for Each Comedian')\n",
    "plt.xlabel('Script Part')\n",
    "plt.ylabel('Polarity')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "topic-modeling",
   "metadata": {},
   "source": [
    "### Topic Modeling Using Gensim LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocess-text",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the scripts\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "df['tokens'] = df['script'].apply(preprocess)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-dictionary-corpus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary and corpus for LDA\n",
    "dictionary = corpora.Dictionary(df['tokens'])\n",
    "corpus = [dictionary.doc2bow(text) for text in df['tokens']]\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = models.LdaModel(corpus, num_topics=4, id2word=dictionary, passes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "print-topics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the topics\n",
    "topics = lda_model.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assign-topics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign topics to each script\n",
    "df['topics'] = [lda_model.get_document_topics(bow) for bow in corpus]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "text-generation",
   "metadata": {},
   "source": [
    "### Text Generation Using Markov Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-markov-chain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a comedian to generate text for\n",
    "comedian_name = 'Ali Wong'  # Change to the comedian you want\n",
    "comedian_scripts = df[df['comedian'] == comedian_name]['script'].str.cat(sep=' ')\n",
    "\n",
    "# Build the Markov Chain\n",
    "def build_markov_chain(text):\n",
    "    words = text.split()\n",
    "    m_chain = defaultdict(list)\n",
    "    for current_word, next_word in zip(words[:-1], words[1:]):\n",
    "        m_chain[current_word].append(next_word)\n",
    "    return dict(m_chain)\n",
    "\n",
    "markov_chain = build_markov_chain(comedian_scripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-text",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text using the Markov Chain\n",
    "def generate_text(chain, count=50):\n",
    "    word1 = random.choice(list(chain.keys()))\n",
    "    sentence = [word1.capitalize()]\n",
    "    for _ in range(count):\n",
    "        word1 = random.choice(chain.get(word1, chain.keys()))\n",
    "        sentence.append(word1)\n",
    "    return ' '.join(sentence) + '.'\n",
    "\n",
    "generated_text = generate_text(markov_chain)\n",
    "print(\"\\nGenerated Text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this notebook, we've performed sentiment analysis, topic modeling, and text generation on a corpus of comedy scripts. We utilized libraries like TextBlob for sentiment analysis, Gensim for topic modeling, and implemented a basic Markov Chain for text generation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
